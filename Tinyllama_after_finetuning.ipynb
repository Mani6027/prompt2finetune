{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPF2omS/kynAn6YcC2l6zn/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kissflow/prompt2finetune/blob/main/Tinyllama_after_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "📚 LEARNING OBJECTIVES:\n",
        "-----------------------\n",
        "In this workshop, you will learn:\n",
        "1. How to fine-tune a language model using LoRA (Low-Rank Adaptation)\n",
        "2. What QLoRA is and how 4-bit quantization saves memory\n",
        "3. How to prepare dataset in ChatML format for training\n",
        "4. How to configure hyperparameters for optimal accuracy\n",
        "5. The difference between pre-training and fine-tuning\n",
        "6. How to evaluate and test your fine-tuned model\n",
        "\n",
        "🎯 WHAT WE'RE BUILDING:\n",
        "-----------------------\n",
        "We're taking a general-purpose TinyLlama model (1.1B parameters) and teaching\n",
        "it specific facts about IPL 2023 cricket matches. After training:\n",
        "- Base model: Doesn't know IPL 2023 facts\n",
        "- Fine-tuned model: Expert on IPL 2023 match results, venues, players\n",
        "\n",
        "🔬 METHODOLOGY:\n",
        "---------------\n",
        "- Technique: QLoRA (Quantized Low-Rank Adaptation)\n",
        "- Base Model: TinyLlama-1.1B (quantized to 4-bit)\n",
        "- Trainable Adapters: LoRA layers (~20-50 MB)\n",
        "- Hardware: NVIDIA A100 GPU (40GB VRAM)\n",
        "- Training Time: ~10-15 minutes\n",
        "- Memory Usage: ~12-15 GB VRAM\n",
        "\n",
        "💡 KEY CONCEPT: FINE-TUNING vs PRE-TRAINING\n",
        "--------------------------------------------\n",
        "PRE-TRAINING: Teaching a model language from scratch (months, massive datasets)\n",
        "FINE-TUNING: Teaching existing model new specific knowledge (minutes, small datasets)\n",
        "\n",
        "Think of it like:\n",
        "- Pre-training = Learning to read and write (years of school)\n",
        "- Fine-tuning = Learning IPL 2023 facts (studying for one exam)"
      ],
      "metadata": {
        "id": "9A3NOw-Pu5v0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# 🔧 STEP 1: INSTALLATION\n",
        "#============================================================================\n",
        "# Install necessary libraries for running the language model\n",
        "\n",
        "# Install 'uv' - A fast Python package installer (faster than pip)\n",
        "!pip install uv\n",
        "\n",
        "# Install Unsloth - A library that optimizes language model training/inference\n",
        "# This is 2-5x faster than standard methods and uses less memory\n",
        "# 'colab-new' flag ensures compatibility with Google Colab's latest environment\n",
        "!uv pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "!uv pip install --no-deps transformers>=4.39.0\n",
        "\n",
        "# Install supporting libraries:\n",
        "# - trl: Transformer Reinforcement Learning (for training)\n",
        "# - peft: Parameter-Efficient Fine-Tuning (LoRA adapters)\n",
        "# - accelerate: Multi-GPU training support\n",
        "# - bitsandbytes: 4-bit/8-bit model quantization for memory efficiency\n",
        "!uv pip install trl peft accelerate bitsandbytes\n",
        "\n",
        "# 💡 KEY TAKEAWAY:\n",
        "# These libraries work together to let us run large models (1.1B parameters!)\n",
        "# on consumer GPUs by using clever memory optimization techniques\n",
        "\n",
        "print(\"✅ Installation complete!\\n\")\n"
      ],
      "metadata": {
        "id": "Ic3GNiwwvGMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# 📦 SECTION 2: IMPORTS & HARDWARE DETECTION\n",
        "#============================================================================\n",
        "# Importing libraries and checking if we have the right GPU\n",
        "\n",
        "import json          # For loading training data (IPL 2023 JSON file)\n",
        "import gc            # Garbage collector (memory management)\n",
        "import torch         # PyTorch (deep learning framework)\n",
        "import warnings      # For suppressing warning messages\n",
        "from pathlib import Path  # For file path handling\n",
        "\n",
        "# 💡 MEMORY CLEANUP\n",
        "# -----------------\n",
        "# Before starting, we clear GPU memory and run garbage collection\n",
        "# This ensures maximum available VRAM for training\n",
        "warnings.filterwarnings(\"ignore\")  # Hide non-critical warnings\n",
        "torch.cuda.empty_cache()           # Clear GPU cache\n",
        "gc.collect()                       # Run Python garbage collector\n",
        "\n",
        "# 💡 CORE LIBRARIES FOR FINE-TUNING\n",
        "# ----------------------------------\n",
        "from unsloth import FastLanguageModel  # Unsloth's optimized model loader\n",
        "from datasets import Dataset            # HuggingFace datasets\n",
        "from trl import SFTTrainer, SFTConfig  # Supervised Fine-Tuning trainer\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Hardware Detection\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "60ZBCRg_vg3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# ⚙️ SECTION 3: TRAINING CONFIGURATION\n",
        "#============================================================================\n",
        "# This is where we define ALL hyperparameters for training\n",
        "\n",
        "# 💡 WHY USE A CONFIG CLASS?\n",
        "# ---------------------------\n",
        "# Organizing all settings in one place makes it easy to:\n",
        "# 1. See all hyperparameters at a glance\n",
        "# 2. Modify settings without searching through code\n",
        "# 3. Share configurations with team members\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Training configuration optimized for A100 GPU.\"\"\"\n",
        "\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    # 📁 FILE PATHS\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "\n",
        "    RAW_JSON = \"/content/ipl_2023_training_data_20251020_194537.json\"\n",
        "    # ↑ Input: Your IPL 2023 training data (instruction-output pairs)\n",
        "\n",
        "    OUTPUT_DIR = \"tinyllama-ipl-unsloth-a100\"\n",
        "    # ↑ Output: Where to save checkpoints and final model\n",
        "\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    # 🤖 MODEL CONFIGURATION\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "\n",
        "    BASE_MODEL = \"unsloth/tinyllama\"\n",
        "    # ↑ Unsloth's optimized TinyLlama (1.1B parameters)\n",
        "    # Alternative: \"unsloth/tinyllama-chat-bnb-4bit\" (pre-quantized)\n",
        "\n",
        "    MAX_SEQ_LENGTH = 2048\n",
        "    # ↑ Maximum tokens per training example\n",
        "    # 💡 WHY 2048?\n",
        "    # - Fits most Q&A pairs without truncation\n",
        "    # - A100 has enough VRAM for this length\n",
        "    # - Longer = better context, but uses more memory\n",
        "    # Comparison: T4 GPU would use 512-1024 (less VRAM)\n",
        "\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    # 🧠 LoRA CONFIGURATION (The Secret Sauce!)\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "\n",
        "    # 💡 WHAT IS LoRA?\n",
        "    # ----------------\n",
        "    # Instead of updating ALL 1.1B parameters (expensive!), we:\n",
        "    # 1. Freeze the base model (1.1B params stay unchanged)\n",
        "    # 2. Add small \"adapter\" layers (only ~20M trainable params)\n",
        "    # 3. Train only the adapters (99% less memory!)\n",
        "    #\n",
        "    # Analogy: Instead of rewriting entire encyclopedia, just add sticky notes\n",
        "\n",
        "    LORA_R = 32\n",
        "    # ↑ LoRA Rank (adapter size)\n",
        "    # 💡 KEY HYPERPARAMETER FOR QUALITY!\n",
        "    # - Higher rank = more capacity to learn facts\n",
        "    # - Rank 32 = HIGH capacity (recommended for accuracy)\n",
        "    # - Rank 8 = LOW capacity (faster but may miss facts)\n",
        "    # - Rank 64 = VERY HIGH (diminishing returns)\n",
        "    #\n",
        "    # Training impact:\n",
        "    # - Rank 8:  ~10M trainable params, faster, lower quality\n",
        "    # - Rank 32: ~40M trainable params, slower, BEST quality ⭐\n",
        "    # - Rank 64: ~80M trainable params, slower, marginal gains\n",
        "\n",
        "    LORA_ALPHA = 64\n",
        "    # ↑ LoRA scaling factor\n",
        "    # 💡 STANDARD PRACTICE: alpha = 2 × rank\n",
        "    # - Controls how much LoRA influences output\n",
        "    # - Higher alpha = stronger adapter influence\n",
        "    # - Always set to 2×rank for balanced training\n",
        "\n",
        "    LORA_DROPOUT = 0.05\n",
        "    # ↑ Dropout probability (regularization)\n",
        "    # 💡 PREVENTS OVERFITTING:\n",
        "    # - 5% of LoRA weights randomly dropped during training\n",
        "    # - Helps model generalize (not just memorize)\n",
        "    # - 0.05 = light regularization (good for small datasets)\n",
        "\n",
        "    TARGET_MODULES = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention layers\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"       # MLP (feedforward) layers\n",
        "    ]\n",
        "    # ↑ Which model layers get LoRA adapters\n",
        "    # 💡 WHY THESE MODULES?\n",
        "    # - q/k/v/o = Attention mechanism (understanding context)\n",
        "    # - gate/up/down = Feedforward layers (knowledge storage)\n",
        "    # - Training ALL 7 modules = maximum accuracy\n",
        "    # - Alternative: Just [\"q_proj\", \"v_proj\"] = faster but lower quality\n",
        "\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    # 🏋️ TRAINING HYPERPARAMETERS\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "\n",
        "    BATCH_SIZE = 8\n",
        "    # ↑ Number of examples processed simultaneously\n",
        "    # 💡 BATCH SIZE TRADE-OFFS:\n",
        "    # - Larger batch = faster training, more VRAM\n",
        "    # - Smaller batch = slower training, less VRAM\n",
        "    # - Batch 8 = optimal for A100 (40GB VRAM)\n",
        "    # - T4 GPU would use batch 2-4 (16GB VRAM)\n",
        "\n",
        "    GRADIENT_ACCUMULATION_STEPS = 4\n",
        "    # ↑ Accumulate gradients over N steps before updating\n",
        "    # 💡 EFFECTIVE BATCH SIZE = BATCH_SIZE × GRADIENT_ACCUMULATION_STEPS\n",
        "    # - Real batch: 8\n",
        "    # - Gradient accumulation: 4\n",
        "    # - Effective batch: 8 × 4 = 32 ⭐\n",
        "    #\n",
        "    # WHY THIS MATTERS:\n",
        "    # - Effective batch 32 = stable training (smooth loss curve)\n",
        "    # - Simulates larger batch without VRAM cost\n",
        "    # - Update frequency: Every 4 steps instead of every step\n",
        "\n",
        "    NUM_EPOCHS = 7\n",
        "    # ↑ Number of complete passes through training data\n",
        "    # 💡 EPOCH GUIDELINES:\n",
        "    # - Too few (1-2) = underfitting (doesn't learn enough)\n",
        "    # - Just right (5-10) = good memorization of facts\n",
        "    # - Too many (20+) = overfitting (memorizes noise)\n",
        "    # - 7 epochs = sweet spot for small datasets like IPL 2023\n",
        "\n",
        "    LEARNING_RATE = 2e-4\n",
        "    # ↑ How big each training step is\n",
        "    # 💡 LEARNING RATE EXPLAINED:\n",
        "    # - 2e-4 = 0.0002 (standard for LoRA fine-tuning)\n",
        "    # - Too high (1e-3) = unstable, loss explodes\n",
        "    # - Too low (1e-5) = too slow, doesn't learn\n",
        "    # - LoRA uses higher LR than full fine-tuning (less params to update)\n",
        "\n",
        "    WARMUP_RATIO = 0.1\n",
        "    # ↑ Percentage of training steps for learning rate warmup\n",
        "    # 💡 WHY WARMUP?\n",
        "    # - Start with low LR, gradually increase to LEARNING_RATE\n",
        "    # - 10% warmup = first 10% of training steps\n",
        "    # - Prevents early instability (sudden large updates)\n",
        "    # - Think: Stretching before running (prevents injury)\n",
        "\n",
        "    WEIGHT_DECAY = 0.01\n",
        "    # ↑ L2 regularization strength\n",
        "    # 💡 PREVENTS OVERFITTING:\n",
        "    # - Penalizes large weights\n",
        "    # - 0.01 = light regularization (standard)\n",
        "    # - Keeps model weights small and generalizable\n",
        "\n",
        "    MAX_GRAD_NORM = 1.0\n",
        "    # ↑ Gradient clipping threshold\n",
        "    # 💡 STABILITY MECHANISM:\n",
        "    # - If gradients exceed 1.0, scale them down\n",
        "    # - Prevents \"exploding gradients\" (training crash)\n",
        "    # - Essential for FP16/BF16 mixed precision training\n",
        "    # - Think: Speed limit for parameter updates\n",
        "\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    # 📊 EVALUATION & MONITORING\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "\n",
        "    TRAIN_TEST_SPLIT = 0.1\n",
        "    # ↑ Percentage of data held out for validation\n",
        "    # 💡 WHY VALIDATION SPLIT?\n",
        "    # - 90% training, 10% validation\n",
        "    # - Validation tracks if model is overfitting\n",
        "    # - Never train on validation data (keeps it honest)\n",
        "\n",
        "    EVAL_STEPS = 25\n",
        "    # ↑ Evaluate on validation set every N training steps\n",
        "    # 💡 MONITORING PROGRESS:\n",
        "    # - Every 25 steps, check validation loss\n",
        "    # - Helps catch overfitting early\n",
        "    # - Can see if model is improving\n",
        "\n",
        "    SAVE_STEPS = 50\n",
        "    # ↑ Save checkpoint every N steps\n",
        "    # 💡 CHECKPOINTING:\n",
        "    # - Saves model state every 50 steps\n",
        "    # - Can resume if training crashes\n",
        "    # - Keep best checkpoints (see save_total_limit)\n",
        "\n",
        "    LOGGING_STEPS = 5\n",
        "    # ↑ Log training metrics every N steps\n",
        "    # 💡 WHAT GETS LOGGED:\n",
        "    # - Training loss (how well model fits training data)\n",
        "    # - Learning rate (changes during warmup/decay)\n",
        "    # - Speed (samples/second)\n",
        "\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    # ⚡ PERFORMANCE OPTIMIZATIONS\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "\n",
        "    USE_GRADIENT_CHECKPOINTING = True\n",
        "    # ↑ Trade compute for memory\n",
        "    # 💡 HOW IT WORKS:\n",
        "    # - Don't store all activations in memory\n",
        "    # - Recompute them during backward pass\n",
        "    # - Slower training (~20%) but 50% less VRAM\n",
        "    # - Essential for large models/sequences\n",
        "\n",
        "    USE_FLASH_ATTENTION = True\n",
        "    # ↑ Use optimized attention implementation\n",
        "    # 💡 FLASH ATTENTION 2:\n",
        "    # - 2-5x faster attention computation\n",
        "    # - Less memory usage\n",
        "    # - Supported by Unsloth out of the box\n",
        "    # - No accuracy loss (just faster math)\n",
        "\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    # 🎲 REPRODUCIBILITY\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "\n",
        "    RANDOM_SEED = 42\n",
        "    # ↑ Random seed for reproducible results\n",
        "    # 💡 WHY SET SEED?\n",
        "    # - Makes training deterministic (same results every run)\n",
        "    # - 42 = traditional ML random seed (Hitchhiker's Guide reference)\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# Print configuration summary\n",
        "print(\"=\"*80)\n",
        "print(\"Configuration Summary\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Model: {config.BASE_MODEL}\")\n",
        "print(f\"LoRA Rank: {config.LORA_R} (high capacity for accuracy)\")\n",
        "print(f\"Batch Size: {config.BATCH_SIZE}\")\n",
        "print(f\"Effective Batch: {config.BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"Epochs: {config.NUM_EPOCHS}\")\n",
        "print(f\"Max Sequence Length: {config.MAX_SEQ_LENGTH} tokens\")\n",
        "print(f\"Target Modules: {len(config.TARGET_MODULES)} (attention + MLP)\")\n",
        "print(f\"Validation Split: {config.TRAIN_TEST_SPLIT * 100}%\")\n",
        "print(\"=\"*80 + \"\\n\")"
      ],
      "metadata": {
        "id": "6woU2mCBviPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# 📚 SECTION 4: DATA LOADING & PREPROCESSING\n",
        "#============================================================================\n",
        "# Loading IPL 2023 dataset and converting to ChatML format\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Loading IPL 2023 Dataset\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 💡 DATASET FORMAT\n",
        "# -----------------\n",
        "# Your JSON file contains instruction-output pairs:\n",
        "# {\n",
        "#   \"instruction\": \"Who won when Gujarat Titans played Chennai Super Kings?\",\n",
        "#   \"output\": \"Chennai Super Kings won by 5 wickets\"\n",
        "# }\n",
        "\n",
        "with open(config.RAW_JSON, encoding=\"utf-8\") as f:\n",
        "    rows = json.load(f)\n",
        "\n",
        "print(f\"✅ Loaded {len(rows)} examples\")\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "# 🎯 CRITICAL SECTION: ChatML FORMAT CONVERSION\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def to_chatml(r):\n",
        "    \"\"\"\n",
        "    Convert instruction-output pair to ChatML format.\n",
        "\n",
        "    💡 WHAT IS ChatML?\n",
        "    ------------------\n",
        "    ChatML (Chat Markup Language) is TinyLlama's conversation format.\n",
        "    It uses special tokens to separate user messages from assistant responses.\n",
        "\n",
        "    Format Structure:\n",
        "    -----------------\n",
        "    <|user|>          ← User message starts\n",
        "    {question}\n",
        "    </s>              ← End of user message\n",
        "    <|assistant|>     ← Assistant response starts\n",
        "    {answer}\n",
        "    </s>              ← End of assistant message\n",
        "\n",
        "    💡 WHY THIS FORMAT?\n",
        "    -------------------\n",
        "    1. Model was pre-trained with this format (TinyLlama-Chat)\n",
        "    2. Special tokens help model distinguish roles (user vs assistant)\n",
        "    3. Consistent format = better learning\n",
        "\n",
        "    💡 COMPARISON WITH OTHER FORMATS:\n",
        "    ---------------------------------\n",
        "    TinyLlama (ChatML):\n",
        "        <|user|>\n",
        "        What is IPL?</s>\n",
        "        <|assistant|>\n",
        "        Indian Premier League</s>\n",
        "\n",
        "    Mistral (INST format):\n",
        "        <s>[INST] What is IPL? [/INST]Indian Premier League</s>\n",
        "\n",
        "    Llama (INST format):\n",
        "        <s>[INST] What is IPL? [/INST] Indian Premier League </s>\n",
        "\n",
        "    ⚠️ IMPORTANT: Always use the correct format for your model!\n",
        "    Using wrong format = model won't learn properly!\n",
        "    \"\"\"\n",
        "    return (f\"<|user|>\\n{r['instruction']}</s>\\n\"\n",
        "            f\"<|assistant|>\\n{r['output']}</s>\\n\")\n",
        "\n",
        "# 💡 CREATE HUGGINGFACE DATASET\n",
        "# ------------------------------\n",
        "# Convert list of dictionaries to HuggingFace Dataset object\n",
        "# This enables efficient batching and processing during training\n",
        "full_dataset = Dataset.from_dict({\n",
        "    \"text\": [to_chatml(r) for r in rows]\n",
        "})"
      ],
      "metadata": {
        "id": "c0AeZ4BzvpVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "# 📊 TRAIN/VALIDATION SPLIT\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "\n",
        "# 💡 WHY SPLIT DATA?\n",
        "# ------------------\n",
        "# Training data: Model learns from this (90%)\n",
        "# Validation data: Monitor overfitting (10%)\n",
        "\n",
        "# Think of it like:\n",
        "# - Training set = Practice problems for exam\n",
        "# - Validation set = Mock test (measure if you're ready)\n",
        "\n",
        "dataset_split = full_dataset.train_test_split(\n",
        "    test_size=config.TRAIN_TEST_SPLIT,  # 10% for validation\n",
        "    seed=config.RANDOM_SEED              # Reproducible split\n",
        ")\n",
        "\n",
        "train_dataset = dataset_split[\"train\"]\n",
        "eval_dataset = dataset_split[\"test\"]\n",
        "\n",
        "print(f\"✅ Training samples: {len(train_dataset)}\")\n",
        "print(f\"✅ Validation samples: {len(eval_dataset)}\")\n",
        "print(f\"\\n📝 Sample formatted text:\")\n",
        "print(\"-\" * 80)\n",
        "print(train_dataset[0][\"text\"][:200] + \"...\")\n",
        "print(\"-\" * 80 + \"\\n\")\n",
        "\n",
        "# 💡 KEY TAKEAWAY - DATA PREPROCESSING\n",
        "# -------------------------------------\n",
        "# Correct data formatting is CRITICAL for fine-tuning success!\n",
        "# - Wrong format = model won't learn (garbage in, garbage out)\n",
        "# - ChatML format matches TinyLlama's pre-training format\n",
        "# - Validation split prevents overfitting"
      ],
      "metadata": {
        "id": "dkTeTvzovv5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# 🤖 SECTION 5: MODEL LOADING (QLoRA Magic!)\n",
        "#============================================================================\n",
        "# Loading TinyLlama with 4-bit quantization and Unsloth optimizations\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Loading TinyLlama with Unsloth Optimizations\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "# 💡 WHAT IS QLoRA?\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "#\n",
        "# QLoRA = Quantized Low-Rank Adaptation (memory-efficient fine-tuning)\n",
        "#\n",
        "# Traditional Fine-Tuning:\n",
        "# - Load full model in FP16 (2 bytes/param)\n",
        "# - TinyLlama 1.1B × 2 bytes = 2.2 GB\n",
        "# - Activations + optimizer states = ~20 GB total\n",
        "#\n",
        "# QLoRA Approach:\n",
        "# - Load base model in 4-bit (0.5 bytes/param)\n",
        "# - TinyLlama 1.1B × 0.5 bytes = 550 MB ⭐\n",
        "# - Train LoRA adapters in BF16 (small, ~20M params)\n",
        "# - Activations + optimizer = ~8 GB total\n",
        "#\n",
        "# Memory Savings: 20 GB → 8 GB (60% reduction!)\n",
        "#\n",
        "# 💡 QUANTIZATION BREAKDOWN:\n",
        "# --------------------------\n",
        "# 4-bit Quantization = Storing weights with only 16 possible values\n",
        "# Normal (FP16): 65,536 possible values per weight\n",
        "# 4-bit (NF4): 16 possible values per weight\n",
        "#\n",
        "# How does this work without losing quality?\n",
        "# - NF4 (NormalFloat4) = Information-theoretically optimal 4-bit format\n",
        "# - Designed for normally-distributed weights (like neural networks)\n",
        "# - Double quantization = Quantize the quantization constants too!\n",
        "#\n",
        "# Quality impact: ~1-2% loss vs FP16, BUT LoRA adapters compensate!\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=config.BASE_MODEL,      # \"unsloth/tinyllama\"\n",
        "    max_seq_length=config.MAX_SEQ_LENGTH,  # 2048 tokens\n",
        "    dtype=None,                        # Auto-detect (will use BF16 on A100)\n",
        "    load_in_4bit=True,                # ⭐ Enable 4-bit NF4 quantization\n",
        ")\n",
        "\n",
        "# 💡 WHAT JUST HAPPENED?\n",
        "# ----------------------\n",
        "# 1. Downloaded TinyLlama-1.1B model from HuggingFace\n",
        "# 2. Loaded weights in 4-bit format (NF4 quantization)\n",
        "# 3. Auto-detected BF16 for LoRA adapters (A100 native support)\n",
        "# 4. Initialized model with 2048 max sequence length\n",
        "\n",
        "print(f\"✅ Base model loaded: {config.BASE_MODEL}\")"
      ],
      "metadata": {
        "id": "LPorffnHv7bF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "# 🔤 TOKENIZER CONFIGURATION\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "\n",
        "# 💡 WHAT IS A TOKENIZER?\n",
        "# -----------------------\n",
        "# Converts text → numbers (tokens) that model understands\n",
        "# Example: \"Hello world\" → [15339, 1917]\n",
        "#\n",
        "# Special tokens:\n",
        "# - <|user|>, <|assistant|> = Role markers\n",
        "# - </s> = End of sequence\n",
        "# - pad_token = Padding for batches\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Use </s> for padding\n",
        "tokenizer.padding_side = \"right\"            # Pad on right side\n",
        "\n",
        "# 💡 WHY RIGHT PADDING?\n",
        "# ---------------------\n",
        "# Left padding: [PAD][PAD]Hello  ← Can confuse attention\n",
        "# Right padding: Hello[PAD][PAD] ← Natural for left-to-right models\n",
        "\n",
        "print(f\"✅ Tokenizer configured (vocab size: {len(tokenizer)} tokens)\")\n",
        "\n",
        "# 💡 KEY TAKEAWAY - MODEL LOADING\n",
        "# --------------------------------\n",
        "# QLoRA enables fine-tuning large models on consumer GPUs:\n",
        "# - 4-bit quantization = 75% memory savings\n",
        "# - BF16 LoRA adapters = High-quality trainable parameters\n",
        "# - Unsloth optimizations = 2-5x faster than standard methods\n",
        "#\n",
        "# Memory breakdown for TinyLlama 1.1B on A100:\n",
        "# - Base model (4-bit): ~550 MB\n",
        "# - LoRA adapters (BF16): ~80 MB\n",
        "# - Activations + gradients: ~10 GB\n",
        "# - Total: ~12-15 GB (well within A100's 40GB)\n"
      ],
      "metadata": {
        "id": "Pqia-Sqov_e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# 🧠 SECTION 6: LoRA ADAPTER CONFIGURATION\n",
        "#============================================================================\n",
        "# Adding trainable LoRA layers while keeping base model frozen\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Applying LoRA Adapters (Optimized for Accuracy)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "# 💡 HOW LoRA WORKS (Visual Explanation)\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "#\n",
        "# Traditional Fine-Tuning:\n",
        "# -------------------------\n",
        "# Input → [1.1B parameters] → Output\n",
        "#         ↑ ALL trainable ↑\n",
        "# Memory: ~20 GB | Time: Slow | Disk: 2.2 GB to save\n",
        "#\n",
        "# LoRA Fine-Tuning:\n",
        "# -----------------\n",
        "# Input → [1.1B frozen params] + [20M LoRA params] → Output\n",
        "#         ↑ Not trained ↑       ↑ Only these train ↑\n",
        "# Memory: ~8 GB | Time: Fast | Disk: 40 MB to save\n",
        "#\n",
        "# Mathematical Representation:\n",
        "# ----------------------------\n",
        "# Standard layer: Y = W × X (where W is huge NxM matrix)\n",
        "# LoRA layer: Y = W × X + (B × A) × X\n",
        "#             ↑           ↑\n",
        "#             Frozen      Trainable (low-rank)\n",
        "#\n",
        "# - W: Original weights (frozen, quantized to 4-bit)\n",
        "# - B × A: Low-rank decomposition (rank=32)\n",
        "#   - B: N×32 matrix\n",
        "#   - A: 32×M matrix\n",
        "#   - Total params: N×32 + 32×M << N×M (huge savings!)\n",
        "#\n",
        "# Example for a 4096×4096 layer:\n",
        "# - Full: 4096 × 4096 = 16,777,216 parameters\n",
        "# - LoRA (rank 32): (4096×32) + (32×4096) = 262,144 parameters\n",
        "# - Reduction: 98.4% fewer parameters!\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=config.LORA_R,                    # Rank = 32 (high capacity)\n",
        "    target_modules=config.TARGET_MODULES,  # 7 modules (attention + MLP)\n",
        "    lora_alpha=config.LORA_ALPHA,      # Scaling = 64 (2× rank)\n",
        "    lora_dropout=config.LORA_DROPOUT,  # Dropout = 0.05 (regularization)\n",
        "    bias=\"none\",                        # Don't train bias terms\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Unsloth's optimized checkpointing\n",
        "    random_state=config.RANDOM_SEED,   # Reproducibility\n",
        "    use_rslora=False,                  # Standard LoRA (not rank-stabilized)\n",
        "    loftq_config=None,                 # No LoftQ initialization\n",
        ")\n",
        "\n",
        "# 💡 PARAMETER BREAKDOWN:\n",
        "# -----------------------\n",
        "# r=32: LoRA rank\n",
        "#   - Higher rank = more capacity to learn\n",
        "#   - Rank 32 is HIGH (good for accuracy)\n",
        "#   - Each layer gets 32 additional \"dimensions\" to work with\n",
        "#\n",
        "# target_modules: Which layers get LoRA\n",
        "#   - q_proj, k_proj, v_proj, o_proj: Attention mechanism\n",
        "#   - gate_proj, up_proj, down_proj: Feedforward (MLP) layers\n",
        "#   - Training all 7 = maximum coverage (best accuracy)\n",
        "#\n",
        "# lora_alpha=64: Scaling factor\n",
        "#   - Formula: scaling = alpha / r = 64 / 32 = 2.0\n",
        "#   - This 2x scaling is standard practice\n",
        "#   - Controls how much LoRA influences final output\n",
        "#\n",
        "# lora_dropout=0.05: Regularization\n",
        "#   - Randomly drop 5% of LoRA weights during training\n",
        "#   - Prevents overfitting to training data\n",
        "#\n",
        "# use_gradient_checkpointing=\"unsloth\": Memory optimization\n",
        "#   - Unsloth's custom implementation (faster than standard)\n",
        "#   - Trades 20% compute for 50% memory savings\n",
        "\n",
        "print(\"✅ LoRA adapters applied\")\n",
        "print(f\"✅ Rank: {config.LORA_R} (high capacity for memorizing facts)\")\n",
        "print(f\"✅ Alpha: {config.LORA_ALPHA} (scaling factor)\")\n",
        "print(f\"✅ Target modules: {len(config.TARGET_MODULES)} (full model coverage)\")\n",
        "print(f\"✅ Gradient checkpointing: Enabled (Unsloth optimized)\")\n",
        "\n",
        "print(\"\\n📊 Trainable Parameters:\")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# 💡 EXPECTED OUTPUT:\n",
        "# -------------------\n",
        "# trainable params: ~40M (4%)\n",
        "# all params: ~1.1B (100%)\n",
        "# trainable%: ~3-4%\n",
        "#\n",
        "# This means:\n",
        "# - Only 4% of parameters are being trained\n",
        "# - 96% of base model stays frozen (in 4-bit)\n",
        "# - Huge memory and compute savings!\n",
        "\n",
        "# 💡 KEY TAKEAWAY - LoRA ADAPTERS\n",
        "# --------------------------------\n",
        "# LoRA is the \"secret sauce\" of efficient fine-tuning:\n",
        "# 1. Freeze base model (save memory)\n",
        "# 2. Add small trainable adapters (3-4% extra params)\n",
        "# 3. Train only adapters (fast, memory-efficient)\n",
        "# 4. Save only adapters (~40 MB vs 2.2 GB)\n",
        "#\n",
        "# Rank 32 is HIGH for LoRA (usually 8-16), but gives best accuracy\n",
        "# for our IPL 2023 task (memorizing specific facts)."
      ],
      "metadata": {
        "id": "WTXk-H1SwEc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# 🏋️ SECTION 7: TRAINING CONFIGURATION\n",
        "#============================================================================\n",
        "# Configuring the SFTTrainer with optimal hyperparameters for A100\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Configuring Training (A100 Optimized for Accuracy)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "# 💡 WHAT IS SFTTrainer?\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "# SFT = Supervised Fine-Tuning\n",
        "# - Modern trainer from TRL (Transformer Reinforcement Learning) library\n",
        "# - Designed for instruction-following models\n",
        "# - Handles ChatML format automatically\n",
        "# - Built-in evaluation and checkpointing\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "\n",
        "training_args = SFTConfig(\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    # 📁 OUTPUT CONFIGURATION\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    output_dir=config.OUTPUT_DIR,  # Where to save checkpoints\n",
        "\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    # 📚 DATASET CONFIGURATION\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    dataset_text_field=\"text\",  # Column name containing training text\n",
        "    max_length=config.MAX_SEQ_LENGTH,  # 2048 tokens max\n",
        "\n",
        "    packing=False,\n",
        "    # 💡 WHAT IS PACKING?\n",
        "    # -------------------\n",
        "    # Packing=True: Combine multiple short examples into one sequence\n",
        "    #   Example: [Q1+A1][Q2+A2][Q3+A3] all in one 2048-token sequence\n",
        "    #   Pros: More efficient (no wasted padding)\n",
        "    #   Cons: Model might mix up different conversations\n",
        "    #\n",
        "    # Packing=False: One example per sequence ⭐ WE USE THIS\n",
        "    #   Example: [Q1+A1][PAD][PAD]... separate from [Q2+A2][PAD]...\n",
        "    #   Pros: Cleaner learning (no cross-contamination)\n",
        "    #   Cons: Some padding waste\n",
        "    #\n",
        "    # For accuracy on small datasets: packing=False is safer!\n",
        "\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    # 🔢 TRAINING HYPERPARAMETERS\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    num_train_epochs=config.NUM_EPOCHS,  # 7 full passes through data\n",
        "    per_device_train_batch_size=config.BATCH_SIZE,  # 8 examples per GPU\n",
        "    per_device_eval_batch_size=config.BATCH_SIZE,   # 8 examples for validation\n",
        "    gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,  # 4 steps\n",
        "\n",
        "    # 💡 EFFECTIVE BATCH SIZE CALCULATION:\n",
        "    # ------------------------------------\n",
        "    # Real batch: 8 examples\n",
        "    # Gradient accumulation: 4 steps\n",
        "    # Effective batch: 8 × 4 = 32 examples\n",
        "    #\n",
        "    # How it works:\n",
        "    # 1. Process 8 examples → calculate gradients → DON'T update yet\n",
        "    # 2. Process 8 more → accumulate gradients → DON'T update yet\n",
        "    # 3. Process 8 more → accumulate gradients → DON'T update yet\n",
        "    # 4. Process 8 more → accumulate gradients → NOW update with avg of 32!\n",
        "    #\n",
        "    # Why use this?\n",
        "    # - Simulate large batch (32) with small VRAM cost (8)\n",
        "    # - Larger effective batch = more stable training\n",
        "\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    # 🎯 OPTIMIZER CONFIGURATION\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    learning_rate=config.LEARNING_RATE,  # 2e-4 (0.0002)\n",
        "\n",
        "    optim=\"adamw_8bit\",\n",
        "    # 💡 WHAT IS adamw_8bit?\n",
        "    # ----------------------\n",
        "    # AdamW = Adam optimizer with Weight decay (standard for transformers)\n",
        "    # 8-bit = Optimizer states stored in 8-bit (instead of 32-bit)\n",
        "    #\n",
        "    # Memory savings:\n",
        "    # - Standard AdamW: 2× model params for optimizer states\n",
        "    # - 8-bit AdamW: 0.5× model params for optimizer states\n",
        "    # - Reduction: 75% less memory for optimizer!\n",
        "    #\n",
        "    # Quality impact: Negligible (~0.1% difference)\n",
        "\n",
        "    weight_decay=config.WEIGHT_DECAY,  # 0.01 (L2 regularization)\n",
        "    max_grad_norm=config.MAX_GRAD_NORM,  # 1.0 (gradient clipping)\n",
        "\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    # 📉 LEARNING RATE SCHEDULE\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    warmup_ratio=config.WARMUP_RATIO,  # 10% of training\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "\n",
        "    # 💡 LEARNING RATE SCHEDULE (Visual):\n",
        "    # ------------------------------------\n",
        "    #\n",
        "    # Cosine Schedule with Warmup:\n",
        "    #\n",
        "    # LR\n",
        "    # ^\n",
        "    # |     ╱╲\n",
        "    # |    ╱  ╲___\n",
        "    # |   ╱       ╲___\n",
        "    # |  ╱            ╲___\n",
        "    # | ╱                 ╲___\n",
        "    # |╱________________________╲___\n",
        "    # +----------------------------> Steps\n",
        "    # ← Warmup →← Cosine Decay →\n",
        "    # (10%)        (90%)\n",
        "    #\n",
        "    # Phase 1 - Warmup (first 10% of steps):\n",
        "    # - LR: 0 → 2e-4 (gradual increase)\n",
        "    # - Why: Prevents early instability\n",
        "    #\n",
        "    # Phase 2 - Cosine Decay (remaining 90%):\n",
        "    # - LR: 2e-4 → ~0 (smooth decrease)\n",
        "    # - Why: Large steps early (fast learning), small steps late (fine-tuning)\n",
        "\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    # 🔧 PRECISION (Mixed Precision Training)\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    # 💡 BF16 vs FP16:\n",
        "    # ----------------\n",
        "    # FP16 (Float16):\n",
        "    # - Pros: Supported on all modern GPUs\n",
        "    # - Cons: Narrow range (can overflow/underflow)\n",
        "    # - Use case: T4, V100 GPUs\n",
        "    #\n",
        "    # BF16 (BFloat16):\n",
        "    # - Pros: Same range as FP32 (no overflow), native on A100\n",
        "    # - Cons: Only supported on A100/H100\n",
        "    # - Use case: A100, H100 GPUs ⭐ WE USE THIS\n",
        "    #\n",
        "    # Speed comparison on A100:\n",
        "    # - FP32: 1x (baseline)\n",
        "    # - FP16: ~2x faster\n",
        "    # - BF16: ~2x faster + better stability\n",
        "\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    # 📊 LOGGING\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    logging_steps=config.LOGGING_STEPS,  # Log every 5 steps\n",
        "    logging_dir=f\"{config.OUTPUT_DIR}/logs\",  # TensorBoard logs\n",
        "\n",
        "    # 💡 WHAT GETS LOGGED?\n",
        "    # --------------------\n",
        "    # Every 5 steps:\n",
        "    # - Training loss (how well model fits data)\n",
        "    # - Learning rate (changes during warmup/decay)\n",
        "    # - Throughput (samples per second)\n",
        "    # - VRAM usage\n",
        "\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    # 💾 CHECKPOINTING STRATEGY\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    save_strategy=\"steps\",           # Save based on steps (not epochs)\n",
        "    save_steps=config.SAVE_STEPS,    # Save every 50 steps\n",
        "    save_total_limit=3,              # Keep only best 3 checkpoints\n",
        "\n",
        "    # 💡 WHY LIMIT CHECKPOINTS?\n",
        "    # -------------------------\n",
        "    # Each checkpoint = ~50 MB\n",
        "    # Unlimited checkpoints = hundreds of GB wasted\n",
        "    # Keep best 3 = enough to recover from crashes, saves disk space\n",
        "\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    # 📈 EVALUATION STRATEGY\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    eval_strategy=\"steps\",                    # Evaluate based on steps\n",
        "    eval_steps=config.EVAL_STEPS,            # Evaluate every 25 steps\n",
        "    load_best_model_at_end=True,             # Load best checkpoint after training\n",
        "    metric_for_best_model=\"eval_loss\",       # Use validation loss as metric\n",
        "    greater_is_better=False,                 # Lower loss = better\n",
        "\n",
        "    # 💡 BEST MODEL SELECTION:\n",
        "    # ------------------------\n",
        "    # During training:\n",
        "    # - Step 25: eval_loss = 1.2 (save checkpoint)\n",
        "    # - Step 50: eval_loss = 0.8 (save, new best!)\n",
        "    # - Step 75: eval_loss = 0.9 (worse, don't save)\n",
        "    # - Step 100: eval_loss = 0.7 (save, new best!)\n",
        "    #\n",
        "    # After training:\n",
        "    # - Load checkpoint from step 100 (best eval_loss = 0.7)\n",
        "    # - Discard final model if it overfitted\n",
        "\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    # 🎲 REPRODUCIBILITY\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    seed=config.RANDOM_SEED,  # 42 (deterministic training)\n",
        "\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    # 🔧 MISCELLANEOUS\n",
        "    # ═══════════════════════════════════════════════════════════════════\n",
        "    report_to=\"none\",  # Don't send metrics to external services (WandB, etc.)\n",
        "\n",
        "    group_by_length=True,\n",
        "    # 💡 WHAT IS group_by_length?\n",
        "    # ----------------------------\n",
        "    # Group examples of similar length into batches\n",
        "    # - Reduces padding waste\n",
        "    # - Faster training (less computation on padding)\n",
        "    # - Example: [Q1(100 tokens), Q2(110 tokens)] in one batch\n",
        "    #            [Q3(500 tokens), Q4(490 tokens)] in another batch\n",
        ")\n",
        "\n",
        "print(f\"✅ Batch size: {config.BATCH_SIZE}\")\n",
        "print(f\"✅ Gradient accumulation: {config.GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"✅ Effective batch size: {config.BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"✅ Learning rate: {config.LEARNING_RATE}\")\n",
        "print(f\"✅ Warmup ratio: {config.WARMUP_RATIO}\")\n",
        "print(f\"✅ LR schedule: Cosine with warmup\")\n",
        "print(f\"✅ Precision: BF16 (A100 optimized)\")\n",
        "print(f\"✅ Evaluation: Every {config.EVAL_STEPS} steps\")\n",
        "print(f\"✅ Best model selection: Enabled (based on validation loss)\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# 💡 KEY TAKEAWAY - TRAINING CONFIGURATION\n",
        "# -----------------------------------------\n",
        "# This configuration balances speed and accuracy:\n",
        "# - Effective batch 32: Stable gradient updates\n",
        "# - BF16 precision: 2x faster on A100\n",
        "# - Cosine schedule: Smooth learning rate decay\n",
        "# - Evaluation every 25 steps: Catch overfitting early\n",
        "# - Best model selection: Use checkpoint with lowest validation loss\n",
        "#\n",
        "# Expected training behavior:\n",
        "# - Loss starts high (~2.0)\n",
        "# - Drops quickly in first epoch (~1.0)\n",
        "# - Gradually decreases to ~0.3-0.5\n",
        "# - Validation loss should track training loss (no overfitting)"
      ],
      "metadata": {
        "id": "2DX9DTuxwJzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# 🚀 SECTION 8: TRAINER INITIALIZATION\n",
        "#============================================================================\n",
        "# Creating the SFTTrainer object with all our configurations\n",
        "\n",
        "print(\"Initializing SFT Trainer...\")\n",
        "\n",
        "# 💡 WHAT DOES SFTTrainer DO?\n",
        "# ---------------------------\n",
        "# The trainer orchestrates the entire training loop:\n",
        "# 1. Batch creation (group examples, add padding)\n",
        "# 2. Forward pass (model predictions)\n",
        "# 3. Loss calculation (how wrong are predictions?)\n",
        "# 4. Backward pass (calculate gradients)\n",
        "# 5. Optimizer step (update LoRA parameters)\n",
        "# 6. Evaluation (check validation loss)\n",
        "# 7. Checkpointing (save best models)\n",
        "# 8. Logging (track metrics)\n",
        "#\n",
        "# All of this happens automatically when we call trainer.train()!\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,                      # Our LoRA-augmented model\n",
        "    processing_class=tokenizer,       # Tokenizer for text→tokens conversion\n",
        "    train_dataset=train_dataset,      # 90% of data (training)\n",
        "    eval_dataset=eval_dataset,        # 10% of data (validation)\n",
        "    args=training_args,               # All hyperparameters from SFTConfig\n",
        ")\n",
        "\n",
        "print(\"✅ Trainer initialized\\n\")\n",
        "\n",
        "# 💡 WHAT HAPPENS DURING INITIALIZATION?\n",
        "# ---------------------------------------\n",
        "# 1. Verify dataset format (check \"text\" field exists)\n",
        "# 2. Setup data collator (handles batching and padding)\n",
        "# 3. Initialize optimizer (AdamW 8-bit)\n",
        "# 4. Setup learning rate scheduler (cosine with warmup)\n",
        "# 5. Prepare model for distributed training (if multi-GPU)\n",
        "# 6. Create output directories for checkpoints and logs\n",
        "\n",
        "#============================================================================\n",
        "# 🎯 SECTION 9: TRAINING (The Main Event!)\n",
        "#============================================================================\n",
        "# Actually running the fine-tuning process\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Starting Training (A100 - Optimized for Accuracy)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(eval_dataset)}\")\n",
        "print(f\"Epochs: {config.NUM_EPOCHS}\")\n",
        "\n",
        "# 💡 CALCULATE TRAINING STEPS\n",
        "# ---------------------------\n",
        "# Steps per epoch = training_samples / effective_batch_size\n",
        "# Total steps = steps_per_epoch × num_epochs\n",
        "steps_per_epoch = len(train_dataset) // (config.BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS)\n",
        "total_steps = steps_per_epoch * config.NUM_EPOCHS\n",
        "\n",
        "print(f\"Steps per epoch: ~{steps_per_epoch}\")\n",
        "print(f\"Total steps: ~{total_steps}\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# 💡 EXAMPLE CALCULATION:\n",
        "# -----------------------\n",
        "# If you have 320 training examples:\n",
        "# - Effective batch: 32\n",
        "# - Steps per epoch: 320 / 32 = 10 steps\n",
        "# - Total steps (7 epochs): 10 × 7 = 70 steps\n",
        "#\n",
        "# Timeline:\n",
        "# - Logging: Every 5 steps (14 logs)\n",
        "# - Evaluation: Every 25 steps (3 evaluations)\n",
        "# - Checkpointing: Every 50 steps (2 checkpoints)\n",
        "\n",
        "# Check initial memory usage\n",
        "if torch.cuda.is_available():\n",
        "    start_memory_gb = torch.cuda.max_memory_reserved() / 1024**3\n",
        "    print(f\"Initial VRAM: {start_memory_gb:.2f} GB\\n\")\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "# 🔥 THE ACTUAL TRAINING HAPPENS HERE\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "\n",
        "# 💡 WHAT HAPPENS DURING trainer.train()?\n",
        "# ----------------------------------------\n",
        "# For each epoch (7 total):\n",
        "#   For each batch (steps_per_epoch):\n",
        "#     1. Load batch of examples\n",
        "#     2. Tokenize text → input IDs\n",
        "#     3. Forward pass: model(input_ids) → predictions\n",
        "#     4. Calculate loss: how different are predictions from targets?\n",
        "#     5. Backward pass: calculate gradients (∂loss/∂parameters)\n",
        "#     6. (Every 4 steps) Update LoRA parameters with accumulated gradients\n",
        "#     7. (Every 5 steps) Log metrics (loss, LR, speed)\n",
        "#     8. (Every 25 steps) Evaluate on validation set\n",
        "#     9. (Every 50 steps) Save checkpoint if best so far\n",
        "#\n",
        "# Expected output during training:\n",
        "# - Step 1: loss ~2.0 (random initialization)\n",
        "# - Step 10: loss ~1.2 (starting to learn)\n",
        "# - Step 25: loss ~0.8 (first evaluation)\n",
        "# - Step 50: loss ~0.5 (first checkpoint)\n",
        "# - Step 70: loss ~0.3 (converged)\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "# 💡 TRAINING COMPLETE!\n",
        "# ---------------------\n",
        "# trainer_stats contains:\n",
        "# - training_loss: Average loss across all steps\n",
        "# - train_runtime: Total training time in seconds\n",
        "# - train_samples_per_second: Throughput metric\n",
        "# - epochs_trained: Confirmation of 7 epochs\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✅ Training Complete!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "xIktSS4SwTVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# 💾 SECTION 10: SAVING THE MODEL\n",
        "#============================================================================\n",
        "# Saving only the LoRA adapters (not the full model)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Saving LoRA Adapters\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 💡 WHAT ARE WE SAVING?\n",
        "# ----------------------\n",
        "# We're NOT saving the entire 1.1B parameter model!\n",
        "# We're ONLY saving the LoRA adapters (~40M parameters)\n",
        "#\n",
        "# Why?\n",
        "# - Base model hasn't changed (still frozen)\n",
        "# - Base model is already on HuggingFace Hub\n",
        "# - Only adapters are new (what we trained)\n",
        "#\n",
        "# File size comparison:\n",
        "# - Full model: ~2.2 GB (1.1B params in FP16)\n",
        "# - LoRA adapters: ~40 MB (40M params in FP16)\n",
        "# - Savings: 98% smaller! ⭐\n",
        "\n",
        "output_path = Path(config.OUTPUT_DIR) / \"lora_adapters\"\n",
        "output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save LoRA adapter weights\n",
        "model.save_pretrained(str(output_path))\n",
        "# This saves: adapter_config.json, adapter_model.safetensors\n",
        "\n",
        "# Save tokenizer (needed for inference)\n",
        "tokenizer.save_pretrained(str(output_path))\n",
        "# This saves: tokenizer.json, tokenizer_config.json, special_tokens_map.json\n",
        "\n",
        "print(f\"✅ LoRA adapters saved to: {output_path}\")\n",
        "print(f\"✅ Adapter size: ~20-50 MB (vs 2.2 GB for full model)\")\n",
        "print(f\"✅ Tokenizer saved\")"
      ],
      "metadata": {
        "id": "shTOBKSJwXqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 💡 SWITCHING TO INFERENCE MODE\n",
        "# -------------------------------\n",
        "# Training mode: Enables dropout, gradient calculation\n",
        "# Inference mode: Disables dropout, no gradients (faster, deterministic)\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "# 🔧 INFERENCE HELPER FUNCTIONS\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def build_prompt(user_msg: str) -> str:\n",
        "    \"\"\"\n",
        "    Build ChatML prompt for inference.\n",
        "\n",
        "    💡 IMPORTANT: Must match training format exactly!\n",
        "    ------------------------------------------------\n",
        "    Training format: <|user|>\\n{question}</s>\\n<|assistant|>\\n{answer}</s>\\n\n",
        "    Inference format: <|user|>\\n{question}</s>\\n<|assistant|>\\n\n",
        "                      ↑ Same prefix, model completes the rest ↑\n",
        "\n",
        "    Why this matters:\n",
        "    - Model was trained to complete text after <|assistant|> token\n",
        "    - Wrong format = model won't generate properly\n",
        "    - Must include </s> after user message (model expects it)\n",
        "    \"\"\"\n",
        "    return f\"<|user|>\\n{user_msg}</s>\\n<|assistant|>\\n\"\n",
        "\n",
        "def extract_answer(full_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract assistant's response from generated text.\n",
        "\n",
        "    Generated text includes prompt + completion:\n",
        "    \"<|user|>\\nQuestion</s>\\n<|assistant|>\\nAnswer</s>\"\n",
        "\n",
        "    We want just the answer part after <|assistant|>.\n",
        "    \"\"\"\n",
        "    if \"<|assistant|>\" not in full_text:\n",
        "        return full_text  # Fallback if format is wrong\n",
        "\n",
        "    # Split at <|assistant|> and take everything after it\n",
        "    ans = full_text.split(\"<|assistant|>\")[-1]\n",
        "\n",
        "    # Remove </s> token and any trailing whitespace\n",
        "    return ans.split(tokenizer.eos_token)[0].strip()\n",
        "\n",
        "def generate_response(question: str, max_new_tokens: int = 120):\n",
        "    \"\"\"\n",
        "    Generate response to a question using the fine-tuned model.\n",
        "\n",
        "    Args:\n",
        "        question: User's question (e.g., \"Who won IPL 2023?\")\n",
        "        max_new_tokens: Maximum tokens to generate (120 = ~90 words)\n",
        "\n",
        "    Returns:\n",
        "        Generated answer text\n",
        "    \"\"\"\n",
        "    # Build ChatML formatted prompt\n",
        "    prompt = build_prompt(question)\n",
        "\n",
        "    # Tokenize: text → token IDs\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # 💡 GENERATION PARAMETERS:\n",
        "    # -------------------------\n",
        "    with torch.no_grad():  # Disable gradient calculation (faster)\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,  # Generate up to 120 new tokens\n",
        "\n",
        "            temperature=0.1,\n",
        "            # 💡 TEMPERATURE: Controls randomness\n",
        "            # - 0.0: Deterministic (always pick highest probability word)\n",
        "            # - 0.1: Very focused (good for factual Q&A) ⭐ WE USE THIS\n",
        "            # - 0.7: Balanced (default for chat)\n",
        "            # - 1.0: Creative (for story generation)\n",
        "            #\n",
        "            # For IPL facts, we want LOW temperature (factual answers)\n",
        "\n",
        "            top_p=0.9,\n",
        "            # 💡 TOP-P (Nucleus Sampling): Alternative to temperature\n",
        "            # - Only sample from top 90% probability mass\n",
        "            # - Filters out very unlikely words\n",
        "            # - 0.9 = standard value\n",
        "\n",
        "            do_sample=True,\n",
        "            # Enable sampling (use temperature/top_p)\n",
        "            # False = greedy decoding (always pick max probability)\n",
        "\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            # Stop generation when </s> token is generated\n",
        "        )\n",
        "\n",
        "    # Decode: token IDs → text\n",
        "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "    # Extract just the answer part\n",
        "    return extract_answer(full_text)"
      ],
      "metadata": {
        "id": "tIbOFr1gwh3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "# 🎯 TEST QUESTIONS\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "\n",
        "test_questions = [\n",
        "    \"In IPL 2023, who won when Gujarat Titans played against Chennai Super Kings?\",\n",
        "    \"Where was the IPL 2023 match between Rajasthan Royals and Sunrisers Hyderabad played?\",\n",
        "    \"Which stadium hosted the Punjab Kings vs Rajasthan Royals match in IPL 2023?\",\n",
        "    \"Who was the Man of the Match when Mumbai Indians played Chennai Super Kings in IPL 2023?\",\n",
        "]\n",
        "\n",
        "# 💡 WHAT TO EXPECT:\n",
        "# ------------------\n",
        "# Before fine-tuning: Model would give generic/wrong answers\n",
        "#   Q: \"Who won Gujarat Titans vs CSK in IPL 2023?\"\n",
        "#   A: \"I don't have information about IPL 2023...\" (base model)\n",
        "#\n",
        "# After fine-tuning: Model should give specific correct answers\n",
        "#   Q: \"Who won Gujarat Titans vs CSK in IPL 2023?\"\n",
        "#   A: \"Chennai Super Kings won by 5 wickets\" (fine-tuned model) ⭐\n",
        "\n",
        "print(\"\\n🧪 Running Test Predictions:\\n\")\n",
        "\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    print(f\"[Test {i}/{len(test_questions)}]\")\n",
        "    print(f\"Q: {question}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Generate answer\n",
        "    answer = generate_response(question)\n",
        "\n",
        "    print(f\"A: {answer}\")\n",
        "    print(\"=\" * 80 + \"\\n\")"
      ],
      "metadata": {
        "id": "xlicqtqxwnuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\"\"\n",
        "╔═══════════════════════════════════════════════════════════════════════╗\n",
        "║                    WHAT YOU'VE LEARNED TODAY                          ║\n",
        "╚═══════════════════════════════════════════════════════════════════════╝\n",
        "\n",
        "1️⃣  FINE-TUNING vs PRE-TRAINING\n",
        "   ✅ Pre-training: Teaching language from scratch (months, billions of tokens)\n",
        "   ✅ Fine-tuning: Teaching specific knowledge (minutes, thousands of examples)\n",
        "\n",
        "2️⃣  QLoRA (Quantized Low-Rank Adaptation)\n",
        "   ✅ 4-bit quantization: 75% memory savings (2.2GB → 550MB base model)\n",
        "   ✅ LoRA adapters: Only train 4% of parameters (1.1B → 40M trainable)\n",
        "   ✅ Memory efficient: Train on single GPU vs 8 GPUs for full fine-tuning\n",
        "\n",
        "3️⃣  HYPERPARAMETER OPTIMIZATION\n",
        "   ✅ LoRA rank: Higher rank = more capacity (8 basic, 32 high, 64 overkill)\n",
        "   ✅ Batch size: Effective batch 32 = stable training\n",
        "   ✅ Learning rate: 2e-4 standard for LoRA, with 10% warmup\n",
        "   ✅ Epochs: 7 epochs sufficient for small dataset memorization\n",
        "\n",
        "4️⃣  DATA FORMATTING\n",
        "   ✅ ChatML format: TinyLlama's conversation structure\n",
        "   ✅ Consistency: Training and inference formats must match!\n",
        "   ✅ Validation split: 10% held out to monitor overfitting\n",
        "\n",
        "5️⃣  EVALUATION & DEPLOYMENT\n",
        "   ✅ Test on specific questions to verify learning\n",
        "   ✅ Save only adapters (~40 MB vs 2.2 GB full model)\n",
        "   ✅ Easy deployment: Load base model + adapters\n",
        "\n",
        "╔═══════════════════════════════════════════════════════════════════════╗\n",
        "║                       KEY NUMBERS TO REMEMBER                         ║\n",
        "╚═══════════════════════════════════════════════════════════════════════╝\n",
        "\n",
        "📊 Model Size:\n",
        "   - Full model: 1.1B parameters (2.2 GB in FP16)\n",
        "   - Quantized: 1.1B parameters (550 MB in 4-bit)\n",
        "   - LoRA adapters: 40M parameters (40 MB in FP16)\n",
        "   - Trainable: 3-4% of total parameters\n",
        "\n",
        "⏱️  Training Time:\n",
        "   - A100 GPU: ~10-15 minutes\n",
        "   - T4 GPU: ~25-35 minutes (with adjusted config)\n",
        "   - Without Unsloth: ~2-3 hours\n",
        "   - Speedup: 8-12x faster!\n",
        "\n",
        "💾 Memory Usage:\n",
        "   - A100 (this config): ~12-15 GB VRAM\n",
        "   - T4 (adjusted): ~10-12 GB VRAM\n",
        "   - Full fine-tuning: ~35-40 GB VRAM\n",
        "   - Savings: 60-70% less memory\n",
        "\n",
        "🎯 Quality Metrics:\n",
        "   - Final training loss: ~0.3-0.5 (excellent)\n",
        "   - Validation loss: Should track training (no overfitting)\n",
        "   - Inference: Specific, factual answers to IPL 2023 questions\n",
        "\n",
        "╔═══════════════════════════════════════════════════════════════════════╗\n",
        "║                    WORKSHOP DISCUSSION POINTS                         ║\n",
        "╚═══════════════════════════════════════════════════════════════════════╝\n",
        "\n",
        "❓ QUESTION 1: Why use LoRA instead of full fine-tuning?\n",
        "   💡 Answer:\n",
        "   - Memory: 60-70% less VRAM (enables training on consumer GPUs)\n",
        "   - Speed: 2-5x faster training\n",
        "   - Storage: 98% smaller adapter files (40 MB vs 2.2 GB)\n",
        "   - Flexibility: Can train multiple adapters for different tasks\n",
        "\n",
        "❓ QUESTION 2: Why is rank 32 better than rank 8 for this task?\n",
        "   💡 Answer:\n",
        "   - Rank = capacity to memorize facts\n",
        "   - IPL 2023 has many specific facts (teams, venues, players, scores)\n",
        "   - Rank 8: Can memorize ~10-20 core facts (basic)\n",
        "   - Rank 32: Can memorize ~100+ facts (comprehensive)\n",
        "   - Trade-off: Rank 32 is 4x slower to train but much more accurate\n",
        "\n",
        "❓ QUESTION 3: What if I want to fine-tune on T4 GPU instead of A100?\n",
        "   💡 Answer:\n",
        "   - Reduce MAX_SEQ_LENGTH: 2048 → 1024 (shorter sequences)\n",
        "   - Reduce BATCH_SIZE: 8 → 2-4 (less memory per batch)\n",
        "   - Increase GRADIENT_ACCUMULATION_STEPS: 4 → 8-16 (keep effective batch 32)\n",
        "   - Consider reducing rank: 32 → 16 (if memory is tight)\n",
        "   - Use FP16 instead of BF16 (T4 doesn't support BF16)\n",
        "   - Expect ~25-35 minutes training time\n",
        "\n",
        "❓ QUESTION 4: How do I know if my model is overfitting?\n",
        "   💡 Answer:\n",
        "   - Check if training loss keeps decreasing but validation loss increases\n",
        "   - Compare answers on validation set vs training set\n",
        "   - If model memorizes exact training examples but fails on similar questions\n",
        "   - Solutions: Increase dropout, reduce epochs, add more training data\n",
        "\n",
        "❓ QUESTION 5: Can I fine-tune on my own dataset?\n",
        "   💡 Answer: YES! Just ensure:\n",
        "   - Format: JSON with \"instruction\" and \"output\" fields\n",
        "   - Conversion: Use to_chatml() function for TinyLlama\n",
        "   - Size: 100-10,000 examples (small datasets work well)\n",
        "   - Quality: Clean, accurate, consistent data\n",
        "   - Validation: Hold out 10% for evaluation\n",
        "\n",
        "╔═══════════════════════════════════════════════════════════════════════╗\n",
        "║                          NEXT STEPS                                   ║\n",
        "╚═══════════════════════════════════════════════════════════════════════╝\n",
        "\n",
        "🚀 EXPERIMENT 1: Try different LoRA ranks\n",
        "   - Rank 8: Fast but basic\n",
        "   - Rank 16: Balanced\n",
        "   - Rank 32: High accuracy (current)\n",
        "   - Rank 64: Overkill (diminishing returns)\n",
        "   - Compare final loss and answer quality\n",
        "\n",
        "🚀 EXPERIMENT 2: Test on your own dataset\n",
        "   - Collect 100-500 question-answer pairs\n",
        "   - Format as {\"instruction\": \"...\", \"output\": \"...\"}\n",
        "   - Run this same script\n",
        "   - Evaluate on held-out questions\n",
        "\n",
        "🚀 EXPERIMENT 3: Try different models\n",
        "   - TinyLlama: 1.1B params (fast, good for learning)\n",
        "   - Mistral-7B: 7B params (better quality, needs more VRAM)\n",
        "   - Llama-3-8B: 8B params (state-of-the-art)\n",
        "   - Note: Each model has different prompt format!\n",
        "\n",
        "🚀 EXPERIMENT 4: Hyperparameter tuning\n",
        "   - Try learning rate: 1e-4, 2e-4, 5e-4\n",
        "   - Try epochs: 3, 5, 7, 10\n",
        "   - Try effective batch size: 16, 32, 64\n",
        "   - Track validation loss to find optimal settings\n",
        "\n",
        "╔═══════════════════════════════════════════════════════════════════════╗\n",
        "║                         RESOURCES                                     ║\n",
        "╚═══════════════════════════════════════════════════════════════════════╝\n",
        "\n",
        "📚 Papers:\n",
        "   - QLoRA: https://arxiv.org/abs/2305.14314\n",
        "   - LoRA: https://arxiv.org/abs/2106.09685\n",
        "   - LLM Fine-tuning: https://arxiv.org/abs/2403.14608\n",
        "\n",
        "🔧 Tools:\n",
        "   - Unsloth: https://github.com/unslothai/unsloth\n",
        "   - HuggingFace PEFT: https://github.com/huggingface/peft\n",
        "   - TRL (SFTTrainer): https://github.com/huggingface/trl\n",
        "\n",
        "💬 Community:\n",
        "   - Unsloth Discord: discord.gg/unsloth\n",
        "   - HuggingFace Forum: discuss.huggingface.co\n",
        "   - r/LocalLLaMA: reddit.com/r/LocalLLaMA\n",
        "\n",
        "╔═══════════════════════════════════════════════════════════════════════╗\n",
        "║                    THANK YOU FOR ATTENDING!                           ║\n",
        "╚═══════════════════════════════════════════════════════════════════════╝\n",
        "\n",
        "Questions? Let's discuss! 🎤\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Workshop script execution complete!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "BRRUml3Ew2vV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}