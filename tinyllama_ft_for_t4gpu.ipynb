{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWEgjsTjnvz0PX2hbTeqQg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kissflow/prompt2finetune/blob/main/tinyllama_ft_for_t4gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGEr3siOGdN6"
      },
      "outputs": [],
      "source": [
        "!pip install uv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ],
      "metadata": {
        "id": "qxr3jlfuGkk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install trl peft accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "rBRGtE2NGmAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from transformers import TrainingArguments, TextStreamer\n",
        "import json\n",
        "import pandas as pd\n",
        "from typing import Dict, List\n",
        "import random"
      ],
      "metadata": {
        "id": "h0Dko-LyGoMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    \"\"\"Training configuration optimized for T4 GPU based on Unsloth documentation.\"\"\"\n",
        "\n",
        "    # Paths\n",
        "    RAW_JSON = \"/content/ipl_2023.json\"\n",
        "    OUTPUT_DIR = \"tinyllama-ipl-unsloth-t45\"\n",
        "\n",
        "    # Model\n",
        "    BASE_MODEL = \"unsloth/tinyllama\"\n",
        "    MAX_SEQ_LENGTH = 512  # ‚ö†Ô∏è CRITICAL: Start with 512, not 1024!\n",
        "\n",
        "    # LoRA (Keep high for quality)\n",
        "    LORA_R = 32\n",
        "    LORA_ALPHA = 64\n",
        "    LORA_DROPOUT = 0.05\n",
        "    TARGET_MODULES = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ]\n",
        "\n",
        "    # üî• CRITICAL T4 SETTINGS (FROM RESEARCH)\n",
        "    BATCH_SIZE = 1  # ‚ö†Ô∏è Research recommends 1 for T4!\n",
        "    GRADIENT_ACCUMULATION_STEPS = 32  # ‚ö†Ô∏è High accumulation!\n",
        "    NUM_EPOCHS = 5  # Research: At least 1 epoch minimum\n",
        "\n",
        "    # üî• LEARNING RATE (Research-backed)\n",
        "    LEARNING_RATE = 2e-4  # Research: Good starting point for QLoRA\n",
        "    WARMUP_RATIO = 0.03  # ‚ö†Ô∏è Research: 3% warmup (not 10%!)\n",
        "    WEIGHT_DECAY = 0.01\n",
        "    MAX_GRAD_NORM = 0.3  # ‚ö†Ô∏è Research: 0.3 is safe default!\n",
        "\n",
        "    # Evaluation\n",
        "    TRAIN_TEST_SPLIT = 0.1\n",
        "    EVAL_STEPS = 25\n",
        "    SAVE_STEPS = 50\n",
        "    LOGGING_STEPS = 5\n",
        "\n",
        "    # Optimization\n",
        "    USE_GRADIENT_CHECKPOINTING = True\n",
        "    USE_FLASH_ATTENTION = True\n",
        "\n",
        "    # Reproducibility\n",
        "    RANDOM_SEED = 42\n",
        "config = Config()\n",
        "\n",
        "\n",
        "\n",
        "#============================================================================\n",
        "# 4. DATA LOADING & PREPROCESSING\n",
        "#============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Loading IPL 2023 Dataset\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "with open(config.RAW_JSON, encoding=\"utf-8\") as f:\n",
        "    rows = json.load(f)\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(rows)} examples\")"
      ],
      "metadata": {
        "id": "py5KEMURGp55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_chatml(r):\n",
        "    \"\"\"Convert instruction-output pair to ChatML format.\"\"\"\n",
        "    return (f\"<|user|>\\n{r['instruction']}</s>\\n\"\n",
        "            f\"<|assistant|>\\n{r['output']}</s>\\n\")\n",
        "\n",
        "# Create dataset\n",
        "full_dataset = Dataset.from_dict({\n",
        "    \"text\": [to_chatml(r) for r in rows]\n",
        "})\n",
        "\n",
        "# Split into train/validation\n",
        "dataset_split = full_dataset.train_test_split(\n",
        "    test_size=config.TRAIN_TEST_SPLIT,\n",
        "    seed=config.RANDOM_SEED\n",
        ")\n",
        "\n",
        "train_dataset = dataset_split[\"train\"]\n",
        "eval_dataset = dataset_split[\"test\"]\n",
        "\n",
        "print(f\"‚úÖ Training samples: {len(train_dataset)}\")\n",
        "print(f\"‚úÖ Validation samples: {len(eval_dataset)}\")\n",
        "print(f\"\\nSample formatted text:\")\n",
        "print(\"-\" * 80)\n",
        "print(train_dataset[0][\"text\"][:200] + \"...\")\n",
        "print(\"-\" * 80 + \"\\n\")"
      ],
      "metadata": {
        "id": "ONA4z_hGHLmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix 1: Suppress torch compilation errors\n",
        "import torch._dynamo\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "\n",
        "# Fix 2: Enable TF32 for better numerical stability\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "# Fix 3: Set manual seed for reproducibility\n",
        "import random\n",
        "import numpy as np\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=config.BASE_MODEL,\n",
        "    max_seq_length=config.MAX_SEQ_LENGTH,\n",
        "    dtype=None,  # Auto-detect (will use BF16 on A100)\n",
        "    load_in_4bit=True,  # 4-bit quantization for efficiency\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Base model loaded: {config.BASE_MODEL}\")\n",
        "print(f\"‚úÖ Precision: BF16 (A100 native support)\")\n",
        "print(f\"‚úÖ Quantization: 4-bit NF4\")\n",
        "\n",
        "# Configure tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(f\"‚úÖ Tokenizer configured (vocab size: {len(tokenizer)})\")"
      ],
      "metadata": {
        "id": "JFholFAdHNrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=config.LORA_R,\n",
        "    target_modules=config.TARGET_MODULES,\n",
        "    lora_alpha=config.LORA_ALPHA,\n",
        "    lora_dropout=config.LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Unsloth's optimized checkpointing\n",
        "    random_state=config.RANDOM_SEED,\n",
        "    use_rslora=False,  # Standard LoRA\n",
        "    loftq_config=None,\n",
        ")\n",
        "\n",
        "\n",
        "print(\"\\nüìä Trainable Parameters:\")\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "ClVtgJAuNlPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = SFTConfig(\n",
        "      output_dir=config.OUTPUT_DIR,\n",
        "\n",
        "      # Dataset\n",
        "      dataset_text_field=\"text\",\n",
        "      max_length=config.MAX_SEQ_LENGTH,  # 512\n",
        "      packing=False,\n",
        "\n",
        "      # Training\n",
        "      num_train_epochs=config.NUM_EPOCHS,\n",
        "      per_device_train_batch_size=config.BATCH_SIZE,  # 1\n",
        "      per_device_eval_batch_size=config.BATCH_SIZE,\n",
        "      gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,  # 32\n",
        "\n",
        "      # üî• OPTIMIZER (CRITICAL FOR T4)\n",
        "      learning_rate=config.LEARNING_RATE,  # 2e-4\n",
        "      optim=\"paged_adamw_8bit\",  # ‚ö†Ô∏è PAGED optimizer for low VRAM!\n",
        "      weight_decay=config.WEIGHT_DECAY,\n",
        "      max_grad_norm=config.MAX_GRAD_NORM,  # 0.3\n",
        "\n",
        "      # üî• SCHEDULER (Research-backed)\n",
        "      warmup_ratio=0.03,  # ‚ö†Ô∏è 3% warmup (crucial!)\n",
        "      lr_scheduler_type=\"cosine\",\n",
        "\n",
        "      # üî• PRECISION (T4 FP16 FIX)\n",
        "      fp16=True,\n",
        "      bf16=False,\n",
        "      fp16_full_eval=True,\n",
        "\n",
        "      # üî• GRADIENT CHECKPOINTING (Essential for T4)\n",
        "      gradient_checkpointing=True,\n",
        "      gradient_checkpointing_kwargs={\n",
        "          'use_reentrant': False  # Better for FP16\n",
        "      },\n",
        "\n",
        "      # Logging\n",
        "      logging_steps=config.LOGGING_STEPS,\n",
        "      logging_dir=f\"{config.OUTPUT_DIR}/logs\",\n",
        "      logging_first_step=True,  # Log first step to catch issues early\n",
        "\n",
        "      # Saving\n",
        "      save_strategy=\"steps\",\n",
        "      save_steps=config.SAVE_STEPS,\n",
        "      save_total_limit=3,\n",
        "\n",
        "      # Evaluation\n",
        "      eval_strategy=\"steps\",\n",
        "      eval_steps=config.EVAL_STEPS,\n",
        "      load_best_model_at_end=True,\n",
        "      metric_for_best_model=\"eval_loss\",\n",
        "      greater_is_better=False,\n",
        "\n",
        "      # Misc\n",
        "      seed=config.RANDOM_SEED,\n",
        "      report_to=\"none\",\n",
        "      group_by_length=True,\n",
        "\n",
        "      # üî• DATALOADER (Important for T4)\n",
        "      dataloader_num_workers=0,  # Avoid CPU overhead\n",
        "      dataloader_pin_memory=True,  # Faster data transfer\n",
        "  )\n",
        "\n",
        "\n",
        "print(\"Initializing SFT Trainer...\")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,  # Validation split\n",
        "    args=training_args,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "RZg1Z9o_HSck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()\n"
      ],
      "metadata": {
        "id": "uN4vTWi6Hp8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "output_path = Path(config.OUTPUT_DIR) / \"lora_adapters\"\n",
        "output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "model.save_pretrained(str(output_path))\n",
        "tokenizer.save_pretrained(str(output_path))"
      ],
      "metadata": {
        "id": "rM4Q4bJtHvgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "def build_prompt(user_msg: str) -> str:\n",
        "    \"\"\"Build ChatML prompt.\"\"\"\n",
        "    return f\"<|user|>\\n{user_msg}</s>\\n<|assistant|>\\n\"\n",
        "\n",
        "def extract_answer(full_text: str) -> str:\n",
        "    \"\"\"Extract assistant's response.\"\"\"\n",
        "    if \"<|assistant|>\" not in full_text:\n",
        "        return full_text\n",
        "    ans = full_text.split(\"<|assistant|>\")[-1]\n",
        "    return ans.split(tokenizer.eos_token)[0].strip()\n",
        "\n",
        "def generate_response(question: str, max_new_tokens: int = 120):\n",
        "    \"\"\"Generate response to a question.\"\"\"\n",
        "    prompt = build_prompt(question)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.1,  # Low temp for factual answers\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "    return extract_answer(full_text)\n",
        "\n",
        "# Test questions\n",
        "test_questions = [\n",
        "    \"In IPL 2023, who won when Gujarat Titans played against Chennai Super Kings?\",\n",
        "    \"Where was the IPL 2023 match between Rajasthan Royals and Sunrisers Hyderabad played?\",\n",
        "    \"Which stadium hosted the Punjab Kings vs Rajasthan Royals match in IPL 2023?\",\n",
        "    \"Who was the Man of the Match when Mumbai Indians played Chennai Super Kings in IPL 2023?\",\n",
        "]"
      ],
      "metadata": {
        "id": "i5Q9pXH8Hxnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nüß™ Running Test Predictions:\\n\")\n",
        "\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    print(f\"[Test {i}/{len(test_questions)}]\")\n",
        "    print(f\"Q: {question}\")\n",
        "    print(\"-\" * 80)\n",
        "    answer = generate_response(question)\n",
        "    print(f\"A: {answer}\")\n",
        "    print(\"=\" * 80 + \"\\n\")"
      ],
      "metadata": {
        "id": "7UE6ZELDH0Xx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}