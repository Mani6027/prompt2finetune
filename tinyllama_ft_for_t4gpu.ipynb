{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPV5IiZrPRF5VA2dlPcUSSa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kissflow/prompt2finetune/blob/main/tinyllama_ft_for_t4gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGEr3siOGdN6"
      },
      "outputs": [],
      "source": [
        "!pip install uv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ],
      "metadata": {
        "id": "qxr3jlfuGkk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install trl peft accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "rBRGtE2NGmAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from transformers import TrainingArguments, TextStreamer\n",
        "import json\n",
        "import pandas as pd\n",
        "from typing import Dict, List\n",
        "import random"
      ],
      "metadata": {
        "id": "h0Dko-LyGoMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "      \"\"\"Training configuration OPTIMIZED for T4 GPU (16GB VRAM).\"\"\"\n",
        "\n",
        "      RAW_JSON = \"/content/ipl_2023.json\"\n",
        "      OUTPUT_DIR = \"tinyllama-ipl-unsloth-t4\"\n",
        "      BASE_MODEL = \"unsloth/tinyllama\"\n",
        "      MAX_SEQ_LENGTH = 1024  # ‚¨áÔ∏è REDUCED from 2048 (T4 memory limit)\n",
        "\n",
        "      LORA_R = 32  # ‚≠ê KEEP HIGH (don't reduce to 8!)\n",
        "      LORA_ALPHA = 64  # 2√ó rank\n",
        "      LORA_DROPOUT = 0.05\n",
        "      TARGET_MODULES = [\n",
        "          \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "          \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "      ]\n",
        "      BATCH_SIZE = 2  # ‚¨áÔ∏è REDUCED from 8 (T4 has less VRAM)\n",
        "      GRADIENT_ACCUMULATION_STEPS = 16  # ‚¨ÜÔ∏è INCREASED from 4 (keep effective batch = 32)\n",
        "      NUM_EPOCHS = 7  # KEEP (need enough training)\n",
        "\n",
        "      # üî• CRITICAL FOR T4: Lower learning rate for FP16 stability\n",
        "      LEARNING_RATE = 1e-4  # ‚¨áÔ∏è HALVED from 2e-4 (FP16 needs lower LR!)\n",
        "\n",
        "      WARMUP_RATIO = 0.1\n",
        "      WEIGHT_DECAY = 0.01\n",
        "      MAX_GRAD_NORM = 0.5  # ‚¨áÔ∏è REDUCED from 1.0 (tighter clipping for FP16)\n",
        "\n",
        "      TRAIN_TEST_SPLIT = 0.1\n",
        "      EVAL_STEPS = 25\n",
        "      SAVE_STEPS = 50\n",
        "      LOGGING_STEPS = 5\n",
        "\n",
        "\n",
        "      USE_GRADIENT_CHECKPOINTING = True\n",
        "      USE_FLASH_ATTENTION = True\n",
        "\n",
        "\n",
        "      RANDOM_SEED = 42\n",
        "config = Config()\n",
        "\n",
        "\n",
        "\n",
        "#============================================================================\n",
        "# 4. DATA LOADING & PREPROCESSING\n",
        "#============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Loading IPL 2023 Dataset\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "with open(config.RAW_JSON, encoding=\"utf-8\") as f:\n",
        "    rows = json.load(f)\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(rows)} examples\")"
      ],
      "metadata": {
        "id": "py5KEMURGp55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_chatml(r):\n",
        "    \"\"\"Convert instruction-output pair to ChatML format.\"\"\"\n",
        "    return (f\"<|user|>\\n{r['instruction']}</s>\\n\"\n",
        "            f\"<|assistant|>\\n{r['output']}</s>\\n\")\n",
        "\n",
        "# Create dataset\n",
        "full_dataset = Dataset.from_dict({\n",
        "    \"text\": [to_chatml(r) for r in rows]\n",
        "})\n",
        "\n",
        "# Split into train/validation\n",
        "dataset_split = full_dataset.train_test_split(\n",
        "    test_size=config.TRAIN_TEST_SPLIT,\n",
        "    seed=config.RANDOM_SEED\n",
        ")\n",
        "\n",
        "train_dataset = dataset_split[\"train\"]\n",
        "eval_dataset = dataset_split[\"test\"]\n",
        "\n",
        "print(f\"‚úÖ Training samples: {len(train_dataset)}\")\n",
        "print(f\"‚úÖ Validation samples: {len(eval_dataset)}\")\n",
        "print(f\"\\nSample formatted text:\")\n",
        "print(\"-\" * 80)\n",
        "print(train_dataset[0][\"text\"][:200] + \"...\")\n",
        "print(\"-\" * 80 + \"\\n\")"
      ],
      "metadata": {
        "id": "ONA4z_hGHLmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=config.BASE_MODEL,\n",
        "    max_seq_length=config.MAX_SEQ_LENGTH,\n",
        "    dtype=None,  # Auto-detect (will use BF16 on A100)\n",
        "    load_in_4bit=True,  # 4-bit quantization for efficiency\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Base model loaded: {config.BASE_MODEL}\")\n",
        "print(f\"‚úÖ Precision: BF16 (A100 native support)\")\n",
        "print(f\"‚úÖ Quantization: 4-bit NF4\")\n",
        "\n",
        "# Configure tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(f\"‚úÖ Tokenizer configured (vocab size: {len(tokenizer)})\")"
      ],
      "metadata": {
        "id": "JFholFAdHNrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=config.LORA_R,\n",
        "    target_modules=config.TARGET_MODULES,\n",
        "    lora_alpha=config.LORA_ALPHA,\n",
        "    lora_dropout=config.LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Unsloth's optimized checkpointing\n",
        "    random_state=config.RANDOM_SEED,\n",
        "    use_rslora=False,  # Standard LoRA\n",
        "    loftq_config=None,\n",
        ")\n",
        "\n",
        "\n",
        "print(\"\\nüìä Trainable Parameters:\")\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "lB3ZrPhHHRhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = SFTConfig(\n",
        "      output_dir=config.OUTPUT_DIR,\n",
        "\n",
        "      dataset_text_field=\"text\",\n",
        "      max_length=config.MAX_SEQ_LENGTH,  # 1024\n",
        "      packing=False,\n",
        "\n",
        "      num_train_epochs=config.NUM_EPOCHS,\n",
        "      per_device_train_batch_size=config.BATCH_SIZE,  # 2\n",
        "      per_device_eval_batch_size=config.BATCH_SIZE,\n",
        "      gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,  # 16\n",
        "\n",
        "      # üî• CRITICAL FOR T4: Lower LR + Better optimizer\n",
        "      learning_rate=config.LEARNING_RATE,  # 1e-4 (not 2e-4!)\n",
        "      optim=\"adamw_8bit\",\n",
        "      weight_decay=config.WEIGHT_DECAY,\n",
        "      max_grad_norm=config.MAX_GRAD_NORM,  # 0.5 (tighter clipping)\n",
        "\n",
        "      warmup_ratio=config.WARMUP_RATIO,\n",
        "      lr_scheduler_type=\"cosine\",\n",
        "\n",
        "      # üî• FP16 CONFIGURATION (T4 doesn't support BF16)\n",
        "      fp16=True,   # ‚≠ê USE FP16 on T4\n",
        "      bf16=False,  # ‚≠ê T4 doesn't support BF16\n",
        "      fp16_full_eval=True,  # Use FP16 for evaluation too\n",
        "\n",
        "      # üî• CRITICAL: Better FP16 handling\n",
        "      gradient_checkpointing=True,\n",
        "      gradient_checkpointing_kwargs={'use_reentrant': False},  # Better for FP16\n",
        "\n",
        "      logging_steps=config.LOGGING_STEPS,\n",
        "      logging_dir=f\"{config.OUTPUT_DIR}/logs\",\n",
        "\n",
        "      save_strategy=\"steps\",\n",
        "      save_steps=config.SAVE_STEPS,\n",
        "      save_total_limit=3,\n",
        "\n",
        "      eval_strategy=\"steps\",\n",
        "      eval_steps=config.EVAL_STEPS,\n",
        "      load_best_model_at_end=True,\n",
        "      metric_for_best_model=\"eval_loss\",\n",
        "      greater_is_better=False,\n",
        "\n",
        "      seed=config.RANDOM_SEED,\n",
        "      report_to=\"none\",\n",
        "      group_by_length=True,\n",
        "  )\n",
        "\n",
        "\n",
        "print(\"Initializing SFT Trainer...\")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,  # Validation split\n",
        "    args=training_args,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "RZg1Z9o_HSck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()\n"
      ],
      "metadata": {
        "id": "uN4vTWi6Hp8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "output_path = Path(config.OUTPUT_DIR) / \"lora_adapters\"\n",
        "output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "model.save_pretrained(str(output_path))\n",
        "tokenizer.save_pretrained(str(output_path))"
      ],
      "metadata": {
        "id": "rM4Q4bJtHvgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "def build_prompt(user_msg: str) -> str:\n",
        "    \"\"\"Build ChatML prompt.\"\"\"\n",
        "    return f\"<|user|>\\n{user_msg}</s>\\n<|assistant|>\\n\"\n",
        "\n",
        "def extract_answer(full_text: str) -> str:\n",
        "    \"\"\"Extract assistant's response.\"\"\"\n",
        "    if \"<|assistant|>\" not in full_text:\n",
        "        return full_text\n",
        "    ans = full_text.split(\"<|assistant|>\")[-1]\n",
        "    return ans.split(tokenizer.eos_token)[0].strip()\n",
        "\n",
        "def generate_response(question: str, max_new_tokens: int = 120):\n",
        "    \"\"\"Generate response to a question.\"\"\"\n",
        "    prompt = build_prompt(question)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.1,  # Low temp for factual answers\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "    return extract_answer(full_text)\n",
        "\n",
        "# Test questions\n",
        "test_questions = [\n",
        "    \"In IPL 2023, who won when Gujarat Titans played against Chennai Super Kings?\",\n",
        "    \"Where was the IPL 2023 match between Rajasthan Royals and Sunrisers Hyderabad played?\",\n",
        "    \"Which stadium hosted the Punjab Kings vs Rajasthan Royals match in IPL 2023?\",\n",
        "    \"Who was the Man of the Match when Mumbai Indians played Chennai Super Kings in IPL 2023?\",\n",
        "]"
      ],
      "metadata": {
        "id": "i5Q9pXH8Hxnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nüß™ Running Test Predictions:\\n\")\n",
        "\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    print(f\"[Test {i}/{len(test_questions)}]\")\n",
        "    print(f\"Q: {question}\")\n",
        "    print(\"-\" * 80)\n",
        "    answer = generate_response(question)\n",
        "    print(f\"A: {answer}\")\n",
        "    print(\"=\" * 80 + \"\\n\")"
      ],
      "metadata": {
        "id": "7UE6ZELDH0Xx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}