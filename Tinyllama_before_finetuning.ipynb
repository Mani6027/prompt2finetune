{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMExHN71j6OYUkfzeNsy0U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kissflow/prompt2finetune/blob/main/Tinyllama_before_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìã PURPOSE:**\n",
        "\n",
        "This notebook demonstrates a baseline test of TinyLlama's knowledge about\n",
        "IPL 2023 cricket tournament BEFORE any fine-tuning. This helps us:\n",
        "1. Understand what the pre-trained model already knows\n",
        "2. Identify knowledge gaps (IPL 2023 facts it doesn't know)\n",
        "3. Establish a baseline to measure improvement after fine-tuning\n",
        "\n",
        "**üéØ LEARNING OBJECTIVES:**\n",
        "- Understand how to load and test large language models\n",
        "- Learn about model quantization for efficient memory usage\n",
        "- Explore prompt formatting and ChatML structure\n",
        "- See the difference between general knowledge and specific domain knowledge\n",
        "- Measure inference performance and response quality\n",
        "\n",
        "**‚öôÔ∏è REQUIREMENTS:**\n",
        "- Google Colab with GPU (T4, A100, or similar)\n",
        "- No prior ML experience needed!\n"
      ],
      "metadata": {
        "id": "f5W4272YtaP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# üîß STEP 1: INSTALLATION\n",
        "#============================================================================\n",
        "# Install necessary libraries for running the language model\n",
        "\n",
        "# Install 'uv' - A fast Python package installer (faster than pip)\n",
        "!pip install uv\n",
        "\n",
        "# Install Unsloth - A library that optimizes language model training/inference\n",
        "# This is 2-5x faster than standard methods and uses less memory\n",
        "# 'colab-new' flag ensures compatibility with Google Colab's latest environment\n",
        "!uv pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "# Install supporting libraries:\n",
        "# - trl: Transformer Reinforcement Learning (for training)\n",
        "# - peft: Parameter-Efficient Fine-Tuning (LoRA adapters)\n",
        "# - accelerate: Multi-GPU training support\n",
        "# - bitsandbytes: 4-bit/8-bit model quantization for memory efficiency\n",
        "!uv pip install trl peft accelerate bitsandbytes\n",
        "\n",
        "# üí° KEY TAKEAWAY:\n",
        "# These libraries work together to let us run large models (1.1B parameters!)\n",
        "# on consumer GPUs by using clever memory optimization techniques"
      ],
      "metadata": {
        "id": "c0-td7EYtnmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# üì¶ STEP 2: IMPORT LIBRARIES & CONFIGURE MODEL\n",
        "#============================================================================\n",
        "\n",
        "from unsloth import FastLanguageModel  # Optimized model loading from Unsloth\n",
        "import torch                           # PyTorch - the deep learning framework\n",
        "import time                            # For measuring inference speed\n",
        "from datetime import datetime          # For timestamps in results\n",
        "\n",
        "# Model configuration constants\n",
        "# Using constants (UPPERCASE) is a Python best practice for values that don't change\n",
        "MODEL_NAME = \"unsloth/tinyllama\"  # TinyLlama: A small but capable 1.1B parameter model\n",
        "MAX_SEQ_LENGTH = 2048             # Maximum number of tokens (words/subwords) the model can process at once\n",
        "\n",
        "# üí° WHY TinyLlama?\n",
        "# - Small enough to run on free Colab GPU (only 1.1B parameters vs 7B+ for larger models)\n",
        "# - Fast inference (< 1 second per response)\n",
        "# - Good baseline for demonstrating fine-tuning improvements\n",
        "\n",
        "print(f\"Loading {MODEL_NAME}...\\n\")"
      ],
      "metadata": {
        "id": "EMhQCG89tx8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# ü§ñ STEP 3: LOAD THE PRE-TRAINED MODEL\n",
        "#============================================================================\n",
        "# Load the TinyLlama model WITHOUT any fine-tuning\n",
        "# This is the \"base\" model trained on general internet text\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,           # Which model to load\n",
        "    max_seq_length=MAX_SEQ_LENGTH,   # How many tokens it can handle\n",
        "    dtype=None,                       # Auto-detect best precision (float16/bfloat16)\n",
        "    load_in_4bit=True,               # ‚≠ê Use 4-bit quantization to save memory\n",
        ")\n",
        "\n",
        "# üí° WHAT IS QUANTIZATION?\n",
        "# Normal models use 16-bit numbers (2 bytes per parameter)\n",
        "# 4-bit quantization uses only 4 bits (0.5 bytes per parameter)\n",
        "# This reduces memory by 75% with minimal quality loss!\n",
        "# Example: 1.1B parameters √ó 2 bytes = 2.2GB (16-bit)\n",
        "#          1.1B parameters √ó 0.5 bytes = 550MB (4-bit)\n",
        "\n",
        "print(\"‚úÖ Base model loaded successfully!\")\n",
        "print(f\"‚úÖ Model: {MODEL_NAME}\")\n",
        "print(f\"‚úÖ Parameters: ~1.1 Billion\")\n",
        "print(f\"‚úÖ Quantization: 4-bit (saves ~75% memory)\")\n",
        "print(f\"‚úÖ Status: Ready for testing (NO fine-tuning applied yet)\")\n",
        "print(\"=\"*80 + \"\\n\")"
      ],
      "metadata": {
        "id": "6yO_4hc5t1dR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# üî§ STEP 4: CONFIGURE THE TOKENIZER\n",
        "#============================================================================\n",
        "# The tokenizer converts text into numbers (tokens) that the model understands\n",
        "\n",
        "# Set padding token to be the same as end-of-sequence token\n",
        "# Padding is used when processing multiple sentences of different lengths\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Set padding to happen on the right side of the text\n",
        "# This matters for how the model processes the input\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# üí° WHAT IS A TOKENIZER?\n",
        "# Language models don't understand text - they understand numbers!\n",
        "# Tokenizer breaks text into pieces (tokens) and converts them to numbers:\n",
        "# Example: \"Hello world!\" ‚Üí [\"Hello\", \" world\", \"!\"] ‚Üí [15496, 1879, 0]\n",
        "\n",
        "\n",
        "#============================================================================\n",
        "# üöÄ STEP 5: ENABLE INFERENCE MODE\n",
        "#============================================================================\n",
        "# Put the model in \"inference mode\" for faster predictions\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# üí° WHAT IS INFERENCE MODE?\n",
        "# Training mode: Model learns by updating weights (slow, uses more memory)\n",
        "# Inference mode: Model just makes predictions (fast, uses less memory)\n",
        "# We're only testing, not training, so inference mode is perfect!"
      ],
      "metadata": {
        "id": "CT5U_e7qt8jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# üìù STEP 6: HELPER FUNCTIONS FOR PROMPTING\n",
        "#============================================================================\n",
        "\n",
        "def build_prompt(user_msg: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert a user question into the proper ChatML format expected by TinyLlama.\n",
        "\n",
        "    ChatML (Chat Markup Language) is a structured format that tells the model:\n",
        "    - What part is the user's question\n",
        "    - Where the model should start its response\n",
        "\n",
        "    Args:\n",
        "        user_msg: The question we want to ask (plain English)\n",
        "\n",
        "    Returns:\n",
        "        Formatted prompt in ChatML structure\n",
        "\n",
        "    Example:\n",
        "        Input:  \"Who won IPL 2023?\"\n",
        "        Output: \"<|user|>\\nWho won IPL 2023?</s>\\n<|assistant|>\\n\"\n",
        "\n",
        "    üí° ChatML Structure Explained:\n",
        "    <|user|>          ‚Üê Marks the start of user's message\n",
        "    {question}</s>    ‚Üê The actual question, ended with </s> (end-of-sequence)\n",
        "    <|assistant|>     ‚Üê Marks where the model should generate its response\n",
        "    \"\"\"\n",
        "    return f\"<|user|>\\n{user_msg}</s>\\n<|assistant|>\\n\"\n",
        "\n",
        "\n",
        "def extract_answer(full_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract just the assistant's response from the full generated text.\n",
        "\n",
        "    The model generates the entire conversation including special tokens.\n",
        "    We only want the actual answer part, not the formatting tokens.\n",
        "\n",
        "    Args:\n",
        "        full_text: Complete generated text with all tokens\n",
        "\n",
        "    Returns:\n",
        "        Just the assistant's answer, cleaned up\n",
        "\n",
        "    Example:\n",
        "        Input:  \"<|user|>\\nQuestion?</s>\\n<|assistant|>\\nThe answer is...</s>\"\n",
        "        Output: \"The answer is...\"\n",
        "\n",
        "    üí° WHY WE NEED THIS:\n",
        "    Models generate more than just the answer - they include the prompt too!\n",
        "    We need to extract just the useful part for display.\n",
        "    \"\"\"\n",
        "    # Check if the assistant marker exists in the text\n",
        "    if \"<|assistant|>\" not in full_text:\n",
        "        return full_text  # Return as-is if no marker found\n",
        "\n",
        "    # Split on \"<|assistant|>\" and take everything after it\n",
        "    ans = full_text.split(\"<|assistant|>\")[-1]\n",
        "\n",
        "    # Remove the end-of-sequence token and any extra whitespace\n",
        "    return ans.split(tokenizer.eos_token)[0].strip()\n",
        "\n",
        "\n",
        "def generate_response(\n",
        "    question: str,\n",
        "    max_new_tokens: int = 120,\n",
        "    temperature: float = 0.1,\n",
        "    show_time: bool = True\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate a response to a question using the language model.\n",
        "\n",
        "    This is the main function that:\n",
        "    1. Formats the question properly\n",
        "    2. Converts text to tokens\n",
        "    3. Runs the model to generate a response\n",
        "    4. Converts tokens back to text\n",
        "    5. Extracts and returns the answer\n",
        "\n",
        "    Args:\n",
        "        question: The question to ask (plain English)\n",
        "        max_new_tokens: Maximum length of response (120 ‚âà 100 words)\n",
        "        temperature: Randomness of response (0.1 = focused, 1.0 = creative)\n",
        "        show_time: Whether to return inference time\n",
        "\n",
        "    Returns:\n",
        "        answer: The model's response\n",
        "        elapsed: Time taken (if show_time=True)\n",
        "\n",
        "    üí° GENERATION PARAMETERS EXPLAINED:\n",
        "    - max_new_tokens: Stop after this many tokens to prevent endless generation\n",
        "    - temperature: Controls randomness\n",
        "        * 0.0 = Always pick most likely word (deterministic)\n",
        "        * 0.1 = Mostly likely words (good for facts)\n",
        "        * 0.7 = Balanced (good for conversation)\n",
        "        * 1.0+ = More random (good for creativity)\n",
        "    - top_p: Only consider words in top 90% probability (nucleus sampling)\n",
        "    - do_sample: Whether to use randomness (True) or always pick top word (False)\n",
        "    \"\"\"\n",
        "    # STEP 1: Format the question into ChatML format\n",
        "    prompt = build_prompt(question)\n",
        "\n",
        "    # STEP 2: Convert text to tokens and move to GPU\n",
        "    # tokenizer() converts text ‚Üí token IDs\n",
        "    # .to(\"cuda\") moves the data to GPU for fast processing\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # STEP 3: Start timing the inference\n",
        "    start_time = time.time()\n",
        "\n",
        "    # STEP 4: Generate response (no gradient calculation needed for inference)\n",
        "    with torch.no_grad():  # Saves memory by not tracking gradients\n",
        "        outputs = model.generate(\n",
        "            **inputs,                                           # Input token IDs\n",
        "            max_new_tokens=max_new_tokens,                    # Maximum response length\n",
        "            temperature=temperature,                           # Randomness (0.1 = focused)\n",
        "            top_p=0.9,                                        # Nucleus sampling threshold\n",
        "            do_sample=True if temperature > 0 else False,     # Enable sampling if temperature > 0\n",
        "            pad_token_id=tokenizer.eos_token_id,             # Token for padding\n",
        "            eos_token_id=tokenizer.eos_token_id,             # Token to end generation\n",
        "        )\n",
        "\n",
        "    # STEP 5: Calculate how long inference took\n",
        "    elapsed = time.time() - start_time\n",
        "\n",
        "    # STEP 6: Convert token IDs back to text\n",
        "    # skip_special_tokens=False keeps formatting tokens like <|assistant|>\n",
        "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "    # STEP 7: Extract just the answer part\n",
        "    answer = extract_answer(full_text)\n",
        "\n",
        "    # STEP 8: Return answer (and time if requested)\n",
        "    if show_time:\n",
        "        return answer, elapsed\n",
        "    return answer"
      ],
      "metadata": {
        "id": "MC95uh-OuCVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# üèè STEP 7: TEST WITH IPL 2023 SPECIFIC QUESTIONS\n",
        "#============================================================================\n",
        "# These questions require knowledge of IPL 2023 tournament\n",
        "# The base model likely WON'T know these answers (they're after its training cutoff)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä TESTING: IPL 2023 Specific Knowledge (Expected: ‚ùå Poor Performance)\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n‚ö†Ô∏è  IMPORTANT: The base model was trained on text from before IPL 2023\")\n",
        "print(\"   It should NOT know specific match results, scores, or statistics.\")\n",
        "print(\"   This demonstrates why fine-tuning is necessary!\\n\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# List of IPL 2023 specific questions\n",
        "# These require exact knowledge of the 2023 tournament\n",
        "ipl_2023_questions = [\n",
        "    \"In IPL 2023, who won when Gujarat Titans played against Chennai Super Kings?\",\n",
        "    \"Where was the IPL 2023 match between Rajasthan Royals and Sunrisers Hyderabad played?\",\n",
        "    \"Which stadium hosted the Punjab Kings vs Rajasthan Royals match in IPL 2023?\",\n",
        "    \"Who was the Man of the Match when Mumbai Indians played Chennai Super Kings in IPL 2023?\",\n",
        "    \"What was the result of the match between Royal Challengers Bangalore and Kolkata Knight Riders in IPL 2023?\",\n",
        "    \"Who won the toss in the Delhi Capitals vs Lucknow Super Giants match in IPL 2023?\",\n",
        "    \"What was the margin of victory when Chennai Super Kings beat Gujarat Titans in IPL 2023?\",\n",
        "    \"Who scored the most runs in the IPL 2023 match between Punjab Kings and Mumbai Indians?\",\n",
        "]\n",
        "\n",
        "# Storage for results\n",
        "results = []        # Will store each question, answer, and time\n",
        "total_time = 0      # Track total time for all questions\n",
        "\n",
        "# Loop through each question and get the model's response\n",
        "for i, question in enumerate(ipl_2023_questions, 1):\n",
        "    # Display question number and text\n",
        "    print(f\"\\n[Question {i}/{len(ipl_2023_questions)}]\")\n",
        "    print(f\"Q: {question}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Generate response with low temperature (0.1) for factual answers\n",
        "    # Low temperature makes the model more confident/deterministic\n",
        "    answer, elapsed = generate_response(question, temperature=0.1)\n",
        "    total_time += elapsed  # Add to running total\n",
        "\n",
        "    # Display the answer and how long it took\n",
        "    print(f\"A: {answer}\")\n",
        "    print(f\"‚è±Ô∏è  Time: {elapsed:.2f}s\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Store results for later analysis\n",
        "    results.append({\n",
        "        \"question\": question,\n",
        "        \"answer\": answer,\n",
        "        \"time\": f\"{elapsed:.2f}s\"\n",
        "    })\n",
        "\n",
        "# Calculate and display average response time\n",
        "avg_time = total_time / len(ipl_2023_questions)\n",
        "\n",
        "print(f\"\\nüìä RESULTS SUMMARY:\")\n",
        "print(f\"   Total questions: {len(ipl_2023_questions)}\")\n",
        "print(f\"   Average response time: {avg_time:.2f}s\")\n",
        "print(f\"   Total test time: {total_time:.2f}s\")\n",
        "\n",
        "# üí° KEY TAKEAWAY:\n",
        "# You'll likely see the model give VAGUE, GENERIC, or INCORRECT answers\n",
        "# This is EXPECTED! The model doesn't have this specific knowledge.\n",
        "# After fine-tuning, these same questions should get accurate, specific answers."
      ],
      "metadata": {
        "id": "gZ7ZmG83uObs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}